{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAXI_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a single game instance\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_policy():\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    return np.random.choice(range(n_actions), n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=200):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    counter = 0\n",
    "    done = False\n",
    "    \n",
    "    while(not done and counter <= t_max):\n",
    "        state, reward, done, _ = env.step(policy[state])\n",
    "        total_reward += reward\n",
    "        counter += 1\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    return float(np.mean([sample_reward(env, policy) for _ in range(n_times)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I Random policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------   L O N G T I M E    -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policies = [get_random_policy() for _ in range(10**3)]\n",
    "# scores = [evaluate(policy) for policy in policies]\n",
    "\n",
    "# best_policy = None\n",
    "# best_score = -float('inf')\n",
    "\n",
    "# best_score = np.max(scores)\n",
    "# best_policy = policies[np.argmax(scores)]\n",
    "\n",
    "# print(\"New best score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -607.97 (LUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II Genetic algorithm \n",
    "\n",
    "The next task is to devise some more effecient way to perform policy search.\n",
    "We'll do that with a bare-bones evolutionary algorithm.\n",
    "[unless you're feeling masochistic and wish to do something entirely different which is bonus points if it works]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    policy = np.zeros_like(policy1)\n",
    "    mask = np.random.choice([0, 1], size=len(policy), p=[1-p, p]) == 1\n",
    "    policy[mask] = policy1[mask]\n",
    "    policy[~mask] = policy2[~mask]\n",
    "    return policy  # P(policy_1_) = p !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.8):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    return crossover(get_random_policy(), policy, p)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_algo(n_epochs, pool_size, n_crossovers, n_mutations, seed, prev_pool=None, prev_scores=None):\n",
    "    np.random.seed(seed)\n",
    "    if not prev_pool:\n",
    "#         policy = np.hstack([np.random.choice(range(4), size=n_states-4), np.array([4,4,5,5])])\n",
    "#         np.random.shuffle(policy)\n",
    "#         print(policy.shape)\n",
    "        pool = [get_random_policy() for _ in range(pool_size)]\n",
    "        pool_scores = [evaluate(policy) for policy in pool]\n",
    "    else:\n",
    "        pool = prev_pool\n",
    "        prev_scores = prev_pool\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch %s:\"%epoch)\n",
    "\n",
    "        to_cross = np.random.choice(range(pool_size), n_crossovers * 2, replace=False)\n",
    "        crossovered = [crossover(pool[to_cross[ix]], pool[to_cross[-ix-1]]) for ix in range(n_crossovers)]\n",
    "\n",
    "        mutated = [mutation(pool[ix]) for ix in np.random.choice(pool_size, n_crossovers, replace=False)]\n",
    "\n",
    "        assert type(crossovered) == type(mutated) == list\n",
    "\n",
    "        #add new policies to the pool\n",
    "        mutated += crossovered\n",
    "        pool += mutated\n",
    "        pool_scores = [evaluate(policy) for policy in pool]\n",
    "\n",
    "        #select pool_size best policies\n",
    "        selected_indices = np.argsort(pool_scores)[-pool_size:]\n",
    "        pool = [pool[i] for i in selected_indices]\n",
    "        pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "        #print the best policy so far (last in ascending score order)\n",
    "        print(\"best score:\", pool_scores[-1])\n",
    "    return pool, pool_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- D O L G O ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "('best score:', -918.29)\n",
      "Epoch 1:\n",
      "('best score:', -988.85)\n",
      "Epoch 2:\n",
      "('best score:', -898.04)\n",
      "Epoch 3:\n",
      "('best score:', -881.66)\n",
      "Epoch 4:\n",
      "('best score:', -792.38)\n",
      "Epoch 5:\n",
      "('best score:', -898.85)\n",
      "Epoch 6:\n",
      "('best score:', -791.75)\n",
      "Epoch 7:\n",
      "('best score:', -846.2)\n",
      "Epoch 8:\n",
      "('best score:', -863.93)\n",
      "Epoch 9:\n",
      "('best score:', -828.83)\n",
      "Epoch 10:\n",
      "('best score:', -827.48)\n",
      "Epoch 11:\n",
      "('best score:', -863.48)\n",
      "Epoch 12:\n",
      "('best score:', -775.46)\n",
      "Epoch 13:\n",
      "('best score:', -828.65)\n",
      "Epoch 14:\n",
      "('best score:', -774.56)\n",
      "Epoch 15:\n",
      "('best score:', -720.83)\n",
      "Epoch 16:\n",
      "('best score:', -649.01)\n",
      "Epoch 17:\n",
      "('best score:', -665.66)\n",
      "Epoch 18:\n",
      "('best score:', -666.83)\n",
      "Epoch 19:\n",
      "('best score:', -737.93)\n",
      "Epoch 20:\n",
      "('best score:', -756.74)\n",
      "Epoch 21:\n",
      "('best score:', -665.66)\n",
      "Epoch 22:\n",
      "('best score:', -630.29)\n",
      "Epoch 23:\n",
      "('best score:', -523.1)\n",
      "Epoch 24:\n",
      "('best score:', -613.19)\n",
      "Epoch 25:\n",
      "('best score:', -577.19)\n",
      "Epoch 26:\n",
      "('best score:', -703.19)\n",
      "Epoch 27:\n",
      "('best score:', -631.19)\n",
      "Epoch 28:\n",
      "('best score:', -612.83)\n",
      "Epoch 29:\n",
      "('best score:', -666.74)\n",
      "Epoch 30:\n",
      "('best score:', -648.38)\n",
      "Epoch 31:\n",
      "('best score:', -631.19)\n",
      "Epoch 32:\n",
      "('best score:', -631.55)\n",
      "Epoch 33:\n",
      "('best score:', -630.92)\n",
      "Epoch 34:\n",
      "('best score:', -594.65)\n",
      "Epoch 35:\n",
      "('best score:', -612.38)\n",
      "Epoch 36:\n",
      "('best score:', -577.01)\n",
      "Epoch 37:\n",
      "('best score:', -469.64)\n",
      "Epoch 38:\n",
      "('best score:', -558.83)\n",
      "Epoch 39:\n",
      "('best score:', -558.74)\n",
      "Epoch 40:\n",
      "('best score:', -522.92)\n",
      "Epoch 41:\n",
      "('best score:', -397.55)\n",
      "Epoch 42:\n",
      "('best score:', -558.83)\n",
      "Epoch 43:\n",
      "('best score:', -523.37)\n",
      "Epoch 44:\n",
      "('best score:', -433.1)\n",
      "Epoch 45:\n",
      "('best score:', -505.19)\n",
      "Epoch 46:\n",
      "('best score:', -523.19)\n",
      "Epoch 47:\n",
      "('best score:', -523.19)\n",
      "Epoch 48:\n",
      "('best score:', -433.55)\n",
      "Epoch 49:\n",
      "('best score:', -469.19)\n",
      "Epoch 50:\n",
      "('best score:', -451.55)\n",
      "Epoch 51:\n",
      "('best score:', -433.19)\n",
      "Epoch 52:\n",
      "('best score:', -450.83)\n",
      "Epoch 53:\n",
      "('best score:', -415.73)\n",
      "Epoch 54:\n",
      "('best score:', -469.82)\n",
      "Epoch 55:\n",
      "('best score:', -433.37)\n",
      "Epoch 56:\n",
      "('best score:', -414.38)\n",
      "Epoch 57:\n",
      "('best score:', -451.01)\n",
      "Epoch 58:\n",
      "('best score:', -451.19)\n",
      "Epoch 59:\n",
      "('best score:', -343.64)\n",
      "Epoch 60:\n",
      "('best score:', -397.19)\n",
      "Epoch 61:\n",
      "('best score:', -361.73)\n",
      "Epoch 62:\n",
      "('best score:', -379.73)\n",
      "Epoch 63:\n",
      "('best score:', -379.46)\n",
      "Epoch 64:\n",
      "('best score:', -397.28)\n",
      "Epoch 65:\n",
      "('best score:', -397.73)\n",
      "Epoch 66:\n",
      "('best score:', -415.1)\n",
      "Epoch 67:\n",
      "('best score:', -343.82)\n",
      "Epoch 68:\n",
      "('best score:', -272.0)\n",
      "Epoch 69:\n",
      "('best score:', -325.82)\n",
      "Epoch 70:\n",
      "('best score:', -343.64)\n",
      "Epoch 71:\n",
      "('best score:', -325.91)\n",
      "Epoch 72:\n",
      "('best score:', -343.46)\n",
      "Epoch 73:\n",
      "('best score:', -379.64)\n",
      "Epoch 74:\n",
      "('best score:', -289.82)\n",
      "Epoch 75:\n",
      "('best score:', -254.0)\n",
      "Epoch 76:\n",
      "('best score:', -307.73)\n",
      "Epoch 77:\n",
      "('best score:', -271.82)\n",
      "Epoch 78:\n",
      "('best score:', -200.0)\n",
      "Epoch 79:\n",
      "('best score:', -325.64)\n",
      "Epoch 80:\n",
      "('best score:', -290.0)\n",
      "Epoch 81:\n",
      "('best score:', -271.91)\n",
      "Epoch 82:\n",
      "('best score:', -289.82)\n",
      "Epoch 83:\n",
      "('best score:', -307.91)\n",
      "Epoch 84:\n",
      "('best score:', -289.91)\n",
      "Epoch 85:\n",
      "('best score:', -290.0)\n",
      "Epoch 86:\n",
      "('best score:', -289.91)\n",
      "Epoch 87:\n",
      "('best score:', -254.0)\n",
      "Epoch 88:\n",
      "('best score:', -289.91)\n",
      "Epoch 89:\n",
      "('best score:', -253.91)\n",
      "Epoch 90:\n",
      "('best score:', -235.82)\n",
      "Epoch 91:\n",
      "('best score:', -236.0)\n",
      "Epoch 92:\n",
      "('best score:', -254.0)\n",
      "Epoch 93:\n",
      "('best score:', -236.0)\n",
      "Epoch 94:\n",
      "('best score:', -236.0)\n",
      "Epoch 95:\n",
      "('best score:', -254.0)\n",
      "Epoch 96:\n",
      "('best score:', -254.0)\n",
      "Epoch 97:\n",
      "('best score:', -253.91)\n",
      "Epoch 98:\n",
      "('best score:', -253.82)\n",
      "Epoch 99:\n",
      "('best score:', -253.91)\n",
      "Epoch 100:\n",
      "('best score:', -253.82)\n",
      "Epoch 101:\n",
      "('best score:', -253.91)\n",
      "Epoch 102:\n",
      "('best score:', -235.73)\n",
      "Epoch 103:\n",
      "('best score:', -236.0)\n",
      "Epoch 104:\n",
      "('best score:', -253.82)\n",
      "Epoch 105:\n",
      "('best score:', -236.0)\n",
      "Epoch 106:\n",
      "('best score:', -200.0)\n",
      "Epoch 107:\n",
      "('best score:', -218.0)\n",
      "Epoch 108:\n",
      "('best score:', -235.91)\n",
      "Epoch 109:\n",
      "('best score:', -236.0)\n",
      "Epoch 110:\n",
      "('best score:', -235.82)\n",
      "Epoch 111:\n",
      "('best score:', -200.0)\n",
      "Epoch 112:\n",
      "('best score:', -218.0)\n",
      "Epoch 113:\n",
      "('best score:', -200.0)\n",
      "Epoch 114:\n",
      "('best score:', -200.0)\n",
      "Epoch 115:\n",
      "('best score:', -200.0)\n",
      "Epoch 116:\n",
      "('best score:', -200.0)\n",
      "Epoch 117:\n",
      "('best score:', -200.0)\n",
      "Epoch 118:\n",
      "('best score:', -218.0)\n",
      "Epoch 119:\n",
      "('best score:', -200.0)\n",
      "Epoch 120:\n",
      "('best score:', -217.91)\n",
      "Epoch 121:\n",
      "('best score:', -200.0)\n",
      "Epoch 122:\n",
      "('best score:', -218.0)\n",
      "Epoch 123:\n",
      "('best score:', -218.0)\n",
      "Epoch 124:\n",
      "('best score:', -200.0)\n",
      "Epoch 125:\n",
      "('best score:', -217.91)\n",
      "Epoch 126:\n",
      "('best score:', -200.0)\n",
      "Epoch 127:\n",
      "('best score:', -200.0)\n",
      "Epoch 128:\n",
      "('best score:', -200.0)\n",
      "Epoch 129:\n",
      "('best score:', -200.0)\n",
      "Epoch 130:\n",
      "('best score:', -217.91)\n",
      "Epoch 131:\n",
      "('best score:', -200.0)\n",
      "Epoch 132:\n",
      "('best score:', -200.0)\n",
      "Epoch 133:\n",
      "('best score:', -200.0)\n",
      "Epoch 134:\n",
      "('best score:', -200.0)\n",
      "Epoch 135:\n",
      "('best score:', -200.0)\n",
      "Epoch 136:\n",
      "('best score:', -200.0)\n",
      "Epoch 137:\n",
      "('best score:', -200.0)\n",
      "Epoch 138:\n",
      "('best score:', -200.0)\n",
      "Epoch 139:\n",
      "('best score:', -200.0)\n",
      "Epoch 140:\n",
      "('best score:', -200.0)\n",
      "Epoch 141:\n",
      "('best score:', -200.0)\n",
      "Epoch 142:\n",
      "('best score:', -200.0)\n",
      "Epoch 143:\n",
      "('best score:', -200.0)\n",
      "Epoch 144:\n",
      "('best score:', -200.0)\n",
      "Epoch 145:\n",
      "('best score:', -200.0)\n",
      "Epoch 146:\n",
      "('best score:', -200.0)\n",
      "Epoch 147:\n",
      "('best score:', -200.0)\n",
      "Epoch 148:\n",
      "('best score:', -200.0)\n",
      "Epoch 149:\n",
      "('best score:', -200.0)\n"
     ]
    }
   ],
   "source": [
    "pool, pool_scores = gen_algo(150, 50, 20, 40, 911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "('best score:', -200.0)\n",
      "Epoch 1:\n",
      "('best score:', -200.0)\n",
      "Epoch 2:\n",
      "('best score:', -200.0)\n",
      "Epoch 3:\n",
      "('best score:', -200.0)\n",
      "Epoch 4:\n",
      "('best score:', -200.0)\n",
      "Epoch 5:\n",
      "('best score:', -200.0)\n",
      "Epoch 6:\n",
      "('best score:', -200.0)\n",
      "Epoch 7:\n",
      "('best score:', -200.0)\n",
      "Epoch 8:\n",
      "('best score:', -200.0)\n",
      "Epoch 9:\n",
      "('best score:', -200.0)\n",
      "Epoch 10:\n",
      "('best score:', -200.0)\n",
      "Epoch 11:\n",
      "('best score:', -200.0)\n",
      "Epoch 12:\n"
     ]
    }
   ],
   "source": [
    "pool, pool_scores = gen_algo(300, 50, 20, 40, 911, pool, pool_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
